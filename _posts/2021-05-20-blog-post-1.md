---
title: 'Expectation Maximization and VAE'
date: 2021-05-20
permalink: /posts/2021/05/blog-post-2/
categories:
  - Blog Posts
tags:
  - VI
  - EM
location: "Singapore"
---
<div align = 'center'>
<img src='/images/VAE03.png' width = "400" >
</div>


## Expectation Maximization

EM is an iterative algorithm with latent variables.

We often find the parameters of the model using the sample that we observed, and use the maximum log likelihood function to get the parameter value. However, in some cases, the observed data we get has unobserved latent variable. Implicit data and model parameters (including latent variable), so it is impossible to directly maximize the log-likelihood function to obtain the parameters of the model distribution.

EM could solve the problem.

<img src='/images/EM_top.png'>
> the Procedure of EM. 


## Variational Auto-encoders (VAE)
 
### Introduction 
VAE is an unsupervised generation algorithm. 
We all know Auto-encoder:
<img src='/images/VAE_autoe.png'>
> Structure of Auto-encoder.  

But in this way, we can't actually generate pictures arbitrarily, because we have no way to construct the latent vector by ourselves. We need to input and encode a picture to know what the hidden vector is. VAE can solve this problem.

VAE add restrictions in the encoding process, forcing the implicit vector generated by it to roughly follow a standard normal distribution. This is the biggest difference between it and the general auto-encoder. 

<img src='/images/vae.png'>
> Structure of VAE.  

EA is a special case of VAE, which simplified the calculation of the q(x,z).   

### Derive  
<img src='/images/vae_derive.png'>
>Derive of the Loss in VAE.

### Result on MINIST  

<img src='/images/vae_result01.png'>
> VAE on MNIST 

<img src='/images/vae_result02.png'>

> VAE on Fashion-MNIST


##  Issues

Status: All good




## Contact
The above is the a brief description of EM and VAE. If you encounter unclear or controversial issues, feel free to contact [Leslie Wong](yushuowang@outlook.com).