---
title: 'Bayesian Neural Network'
date: 2022-04-12
permalink: /posts/2022/4/blog-post-1/
categories:
  - Blog Posts
tags:
  - Bayesian Neural Network
location: "Singapore"
---
<div align = 'center'>
<img src='/images/bayes_mlp.png' width = "500" >
</div>

## Bayesian Neural Network

In contrast to traditional neural networks, Bayesian Neural Networks(BNN)' weights are allocated a probability distribution rather than a single value or point estimate. BNN has the following advantages:

1. Ensemble Model. Because the weights in the Bayesian neural network are allocated a probability distribution, then multiple sampling of BNN on a certain weight distribution can be integrated for prediction, which is equivalent to ensemble.  
2. Uncertainty of the prediction. Multiple sampling on a distribution would help us get the standard deviation of the prediction, which tells us the uncertainty of the prediction.
3. Regularization.

Instead of learning the weights directly, a Bayesian neural network is trained using variational inference to learn the parameters of these distributions(μ and σ when the weights are allocated a Gaussian Distribution):  

<div align = 'center'>
<img src='/images/bayes_mlp.png' width = "500" >
</div>
> Figure 3.1: Left: the weights are fixed during forward propagation. Right: each weight is assigned a distribution.    

A neural network can be considered as a probabilistic model $p(y \vert x,w)$. y is
referred to a set of classes while a categorical distribution is referred to as $p(y \vert x,w)$. y is a continuous variable in regression, while $p(y \vert x,w)$ is a Gaussian distribution.

Given a training dataset D{$x^(i)$, $y^(i)$}. For a neural network, we could construct the likelihood function $p(D \vert \textbf{w}) = \prod_{i} p(y^{i}  \vert \textbf{x}^{i}, \textbf{w})$. which is a function of parameters $\textbf{w}$. Maximizing the likelihood function gives the maximimum likelihood estimate (MLE) of $\textbf{w}$. The usual optimization objective during training is the negative log likelihood. For a categorical distribution this is the cross entropy error function, for a Gaussian distribution this is proportional to the sum of squares error function. MLE can lead to severe overfitting though.

$\theta_{MLE} &= \underset{\theta}{\operatorname{argmax}} log\  p(D \vert w)$
$&= \underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{n} log \ p((y_{i}  \vert x_{i} , w)$

In bayesian's view, we could add a prior $p(w)$ to the MLE:
$\theta_{MAP} &= \underset{\theta}{\operatorname{argmax}}log \ p(w \vert D)$
$&= \underset{\theta}{\operatorname{argmax}} log \ p(D  \vert w) + log\ p(w) - log\ p(D)$
$&= \underset{\theta}{\operatorname{argmax}} log \ p(D  \vert w) + log\ p(w)$

Which $log\ p(w)$ is equivalent to an L2 regularization term (which tends to be a small value).
And when the prior is a Laplace Distribution, $log\ p(w)$ is equivalent to an L1 regularization (tends to be 0 to make the weights sparse).

Both MLE and MAP give point estimates of parameters. If we instead had a full **posterior distribution** over parameters we could make predictions that take weight uncertainty into account. Then the probabilistic model would be:

$p(y  \vert x) &= E_{p(w \vert D)}[p(y \vert x,w)]$
$&= \int_{w}^{} p(y \vert x, w)p(w \vert D)dw$  

Then there are two problems:
1. 
