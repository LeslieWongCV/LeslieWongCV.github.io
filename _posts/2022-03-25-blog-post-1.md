---
title: 'Maximum Likelihood Estimation and Bayesian Estimation'
date: 2022-03-25
permalink: /posts/2022/3/blog-post-1/
categories:
  - Blog Posts
tags:
  - Prob
  - Bayesian
location: "Singapore"
---
<div align = 'center'>
<img src='/images/Bayesian_Formular3.png' width = "500" >
</div>

## Maximum Likelihood Estimation

**The frequentists view** is that the **data is a repeatable random sample** (random variable) with a specific frequency/probability. The underlying parameters and probabilities remain constant during this repeatable process and that the variation is due to variability in X<sub>n</sub> and not the probability distribution (which is fixed for a certain event/process).

Eg:
Define *P(X)* is the **probability density function(PDF)** of *X* with parameter θ. After we got a sequence of observation (X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>) = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>), we would like to get the estimate for parameter θ.

In frequentists view, θ has specific value. Event (X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>) = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>) is most likely to occur when θ = θ-MLE-, which θ<sub>MLE</sub> is the Maximum Likelihood Estimation of θ.

<div align = 'center'>
<img src='/images/Bayeisan_Fomular.png' width = "350" >
</div>

The item that needs to be optimized is Negative Log Likelihood (NLL). 

## Bayesian Estimation


**The bayesian view** is that **the data is fixed** while the frequency/probability for a certain event can change, which means the parameters of the distribution changes. In effect, the data that you get changes the prior distribution of a parameter which gets updated for each set of data. (the prior is changed when using different dataset)


<div align = 'center'>
<img src='/images/Bayesian_Formular2.png' width = "300" >
</div>

**p(θ)**: prior, which is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.   
**p(x|θ)**: likelihood, what would x be when θ is known.  
**p(θ|x)**: posterior, the distribution of the parameter.  
**p(x)**: prior of the dataset, a constant that is independent of the parameter θ to be estimated.  

Bayesian estimation is equivalent to that θ obeys the prior distribution p(θ), and then corrects the prior distribution according to the observed samples, and finally obtains the posterior distribution p(θ∣x), and then takes the posterior distribution Expected as an estimate of the parameter.

Bayesian estimation equals to MLE when p(θ) is Gaussian distribution.


## Maximum a posteriori estimation  

We can calculate the posterior distribution of θ using Bayes' theorem.
**A maximum a posteriori probability (MAP) **estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. 

<div align = 'center'>
<img src='/images/Bayesian_Formular3.png' width = "500" >
</div>

The *-logp(x∣θ)* is Negative Log Likelihood(NLL). Therefore, compared with MLE, MAP is an additional a priori term p(θ) during optimization. In some cases, −logp(θ) can be used as a regularizer in structured risk when using MLE, such as when the prior is a Gaussian distribution:

<div align = 'center'>
<img src='/images/Bayesian_Formular4.png' width = "350" >
</div>

At this time, *−logp(θ)* is equivalent to an **L2 regularization term** (which tends to take a small value).

And when the prior is a Laplace Distribution, *−logp(θ)* is equivalent to an **L1 regularization** (tends to take 0 to make the weights sparse).

MAP provides an intuitive way to design complex but interpretable regularization terms, for example more complex regularization terms can be obtained by taking mixture Gaussian distributions as priors.