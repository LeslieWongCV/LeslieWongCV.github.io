---
title: 'Transformers'
date: 2021-05-23
permalink: /posts/2021/05/blog-post-3/
categories:
  - Blog Posts
tags:
  - Transformers
  - Encoder&Decoder
location: "Singapore"
---
<div align = 'center'>
<img src='/images/transformer.png' width = "400" >
</div>


## Transformers

### Introduction

Transformer is a self-Attenion + Feed Forward Neural Network. It has ResNet architecture inside of it.

### Encoder 

<img src='/images/transformer_acti.png'>  
> Structure of Encoder 


<img src='/images/transQKV.png'>  
> Wq, Wk and Wv

<img src='/images/transMuti.png'>  
> Attention mechanism = Soft addressing  

### BN ==> LN

In Transformers, Layer Normalization is used instead of Batch Normalization, because in NLP problem, the length of a sentence might be different, and BN scaling the data from the feature dimension, which has two problem:

- some feature dimension will be 0 and the performance of BN is BAD.
- small batch_size will also cause BAD performance.
- Scaling from the feature dimension is not reasonable when the sample is a sentence.

<img src='/images/transLN.png'>  

### Decoder

Masked label is used in decoder to ensure that same data is fit in during training and test period.

The K and V for is from encoder.

<img src='/images/transDecoder.png'>  


##  Issues

Status: All good




## Contact
The above is the a brief description of Transformers. If you encounter unclear or controversial issues, feel free to contact [Leslie Wong](yushuowang@outlook.com).