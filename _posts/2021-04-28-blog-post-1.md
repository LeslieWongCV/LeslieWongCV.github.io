---
title: 'Evolution Algorithms: PSO, CLPSO'
date: 2021-04-28
permalink: /posts/2021/04/blog-post-2/
categories:
  - Blog Posts
tags:
  - EE6227
  - Generic Algorithm 
location: "Singapore"
---
<div align = 'center'>
<img src='/images/Func_1[Sphere_function]W0.8.png' width = "400" >
</div>
>Result of CLPSO


## What did I do
I compared two algorithms: Particle Swarm Optimization, Comprehensive Learning Particle Swarm Optimizer. I changed the different hyper parameters for each algorithm with the functions shown above.

Codes are available at: [https://github.com/LeslieWongCV/EE6227_Wong/tree/main/PSO](https://github.com/LeslieWongCV/EE6227_Wong/tree/main/PSO)

## Setup on which the code was tested
- python==3.6
- numpy==1.19.2 


## Particle Swarm Optimization 
Professor P.N. Suganthan concluded: ‘Particle Swarm Optimization(PSO) is a population based search process where individuals never dies.’ 

Indeed, During the optimization, each particle is ‘explore’ to the whole search range of the problem, keep changing its position according to its own experiment and also refer to others. There is no mutation, crossover in PSO.

### Features

1. The sample in PSO has VELOCITY.

### Procedure

1. Similar to DE, initialize the all the particles from range ∈ [Xi_Min, Xi_Max] randomly, where Xi_Min and Xi_max are the lower/upper bound of the particles. i denote the ith dimension of the decision number. Calculate the fitness score of the particles when finished initialization.

2. Initialize the values of the velocity vector from range ∈ [-Vmax, Vmax], where Vmax is the maximum value.
3. For each particle, update the fitness value of the best position of itself (pbest) and also store the best position during the iteration. Select the best particle of all and store both the fitness value and the position as g_best.
4. Refresh the velocity using g_best and the best of each particles as following:<img src='/images/PSO_fomular01.png'>
5. Then update the particles:<img src='/images/PSO_fomular02.png' width = '200' >
6. Continue the procedure until gbest or the iteration number meet the redetermined value, otherwise, keep on searching the potential gbest. Using bigger ω at the start of the iteration and then reduce it during the procedure, since the smaller ω is more appropriate for local search.


## Comprehensive Learning Particle Swarm Optimization(CLPSO)

A improved method of PSO, which updates the principle of refreshing Vid:



<img src='/images/CLPSO_fomular01.png' width = '480'>


### Feature

 Three major differences between CLPSO and the conventional PSO are highlighted:
1. Instead of using a particle’s pbest and swarm’s gbest as the exemplars, all particles’ pbests can be used to guide a particle’s flying direction.
2. Instead of learning from the same pbest for all dimensions, different dimensions of a particle may learn from different virtual pbests within certain generations. In other words, at one iteration, each dimension of a particle may learn from the corresponding dimension of different particle’s pbest.
3. Instead of learning from two exemplars (pbest and gbest) in every generation, each dimension of a particle in CLPSO learns from just one comprehensive exemplar within certain generations.

### Procedure

CLPSO can be summarized as follows:


<img src='/images/CLPSO_pro.png'>



## Results


### PSO

<img src='/images/Func_1[Sphere_function].png' width = '800' >
<img src='/images/Func7.png' width = '800' >


### CLPSO 

<img src='/images/Func_1[Sphere_function]W0.8.png'>
<img src='/images/unc_5[Wei.png'>

##  Issues
When using tournament selection to generate exemplars, instead of taking two particles and comparing them, we shuffled the p_best and chose the top 20% as the set for new exemplars, this measure accelerates the process and also has a good performance. 



## Contact
The above is the a brief description of PSO, CLPSO. If you encounter unclear or controversial issues, feel free to contact [Leslie Wong](yushuowang@outlook.com).